# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "chatbot.baml": "// Conversation history for context\nclass ConversationHistory {\n  messages string[] @description(\"List of previous messages in the conversation\")\n}\n\n// Simple response with just the answer\nclass ChatResponse {\n  answer string @description(\"The chatbot's answer to the user's question\")\n}\n\n// Non-streaming chat function\nfunction Chat(\n  user_question: string,\n  conversation_history: ConversationHistory?\n) -> ChatResponse {\n  client CustomSonnet45\n\n  prompt #\"\n    {{ _.role(\"system\") }}\n    You are a helpful AI assistant. Answer questions clearly and concisely.\n\n    {% if conversation_history %}\n    ## Previous Conversation:\n    {% for msg in conversation_history.messages %}\n    {{ msg }}\n    {% endfor %}\n    {% endif %}\n\n    {{ _.role(\"user\") }}\n    {{ user_question }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Streaming chat function for real-time responses\nfunction StreamChat(\n  user_question: string,\n  conversation_history: ConversationHistory?\n) -> ChatResponse {\n  client CustomSonnet45\n\n  prompt #\"\n    {{ _.role(\"system\") }}\n    You are a helpful AI assistant. Answer questions clearly and concisely.\n\n    {% if conversation_history %}\n    ## Previous Conversation:\n    {% for msg in conversation_history.messages %}\n    {{ msg }}\n    {% endfor %}\n    {% endif %}\n\n    {{ _.role(\"user\") }}\n    {{ user_question }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n",
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\n// Using the new OpenAI Responses API for enhanced formatting\nclient<llm> CustomGPT5 {\n  provider openai\n  options {\n    model \"gpt-5.2\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> Gemini {\n  provider google-ai\n  options {\n    model \"gemini-3-pro-preview\"\n    api_key env.GOOGLE_API_KEY\n    generationConfig {\n      maxOutputTokens 16000\n    }\n  }\n}\n\n// Claude Opus 4.5 - Most capable model (released Nov 2025)\nclient<llm> CustomOpus45 {\n  provider anthropic\n  options {\n    model \"claude-opus-4-5-20251101\"\n    api_key env.ANTHROPIC_API_KEY\n    max_tokens 16000\n  }\n}\n\n// Claude Sonnet 4.5 - Best coding model and strongest for complex agents (released Sep 2025)\nclient<llm> CustomSonnet45 {\n  provider anthropic\n  options {\n    model \"claude-sonnet-4-5-20250929\"\n    api_key env.ANTHROPIC_API_KEY\n    max_tokens 16000\n  }\n}\n\nclient<llm> AzureOpenAIGpt5 {\n  provider azure-openai\n  options {\n    resource_name env.AZURE_OPENAI_RESOURCE_NAME\n    deployment_id env.AZURE_OPENAI_GPT5_DEPLOYMENT_ID\n    api_version env.AZURE_OPENAI_API_VERSION\n    api_key env.AZURE_OPENAI_KEY\n    max_tokens null\n    }\n}\n\n// Main Ollama client for Briscola\n// Uses OLLAMA_BASE_URL env var in Docker, defaults to localhost\nclient<llm> Ollama {\n  provider \"openai-generic\"\n  retry_policy Constant\n  options {\n    base_url env.OLLAMA_BASE_URL ?? \"http://localhost:11434/v1\"\n    model qwen3:4b\n    // Optimize for speed\n    temperature 0.3\n    max_tokens 100\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT5, AzureOpenAIGpt5]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"go\", \"rust\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.217.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
}

def get_baml_files():
    return _file_map